{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model path: person_a_baseline/results/model_weights/resnet18_baseline.pth\n",
      "‚úÖ Data path: data/\n",
      "üì± Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SETUP - YOUR EXACT PATHS\n",
    "# ============================================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# YOUR CONFIRMED PATHS\n",
    "MODEL_PATH = 'person_a_baseline/results/model_weights/resnet18_baseline.pth'\n",
    "DATA_PATH = 'data/'\n",
    "\n",
    "print(f\"‚úÖ Model path: {MODEL_PATH}\")\n",
    "print(f\"‚úÖ Data path: {DATA_PATH}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üì± Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD THE SAVED MODEL\n",
    "# ============================================\n",
    "\n",
    "# Create model architecture\n",
    "def create_model(num_classes=102):\n",
    "    \"\"\"Recreate the ResNet18 model architecture\"\"\"\n",
    "    model = models.resnet18(weights=None)  # No pretrained weights needed\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Initialize and load model\n",
    "model = create_model(num_classes=102)\n",
    "model_state = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(model_state)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully from: person_a_baseline/results/model_weights/resnet18_baseline.pth\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD SAVED MODEL\n",
    "# ============================================\n",
    "\n",
    "# Create model architecture\n",
    "def create_model(num_classes=102):\n",
    "    \"\"\"Recreate the ResNet18 model architecture\"\"\"\n",
    "    model = models.resnet18(pretrained=False)  # Don't need pretrained, we have our weights\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = create_model(num_classes=102)\n",
    "\n",
    "# Load saved weights - UPDATE PATH IF NEEDED\n",
    "try:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model_state = torch.load(MODEL_PATH, map_location=device)\n",
    "        model.load_state_dict(model_state)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"‚úÖ Model loaded successfully from: {MODEL_PATH}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Model file not found at: {MODEL_PATH}\")\n",
    "        print(\"Please update the model_path variable with the correct path to your saved model.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Make sure the model file exists and the path is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded from existing path!\n",
      "üìä Dataset sizes:\n",
      "  Training: 1020 samples\n",
      "  Validation: 1020 samples\n",
      "  Test: 6149 samples\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD EXISTING DATASET - NO DOWNLOAD\n",
    "# ============================================\n",
    "\n",
    "def get_data_transforms():\n",
    "    \"\"\"Get data transforms for evaluation\"\"\"\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    return test_transform\n",
    "\n",
    "# Load datasets from your existing data path\n",
    "test_transform = get_data_transforms()\n",
    "\n",
    "# IMPORTANT: download=False since data already exists\n",
    "train_dataset = datasets.Flowers102(\n",
    "    root=DATA_PATH, \n",
    "    split='train',\n",
    "    transform=test_transform,\n",
    "    download=False  # DO NOT DOWNLOAD - USE EXISTING\n",
    ")\n",
    "\n",
    "val_dataset = datasets.Flowers102(\n",
    "    root=DATA_PATH, \n",
    "    split='val',\n",
    "    transform=test_transform,\n",
    "    download=False  # DO NOT DOWNLOAD - USE EXISTING\n",
    ")\n",
    "\n",
    "test_dataset = datasets.Flowers102(\n",
    "    root=DATA_PATH, \n",
    "    split='test',\n",
    "    transform=test_transform,\n",
    "    download=False  # DO NOT DOWNLOAD - USE EXISTING\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=False, num_workers=0\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=32, shuffle=False, num_workers=0\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data loaded from existing path!\")\n",
    "print(f\"üìä Dataset sizes:\")\n",
    "print(f\"  Training: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL ARCHITECTURE STATISTICS\n",
      "======================================================================\n",
      "Model: ResNet18 (Modified for 102 classes)\n",
      "Total Parameters: 11,228,838\n",
      "Trainable Parameters: 11,228,838\n",
      "Model Size (MB): 42.83\n",
      "Input Size: 3 √ó 224 √ó 224\n",
      "Output Classes: 102\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MODEL STATISTICS\n",
    "# ============================================\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count model parameters\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL ARCHITECTURE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: ResNet18 (Modified for 102 classes)\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Model Size (MB): {total_params * 4 / 1024 / 1024:.2f}\")\n",
    "print(f\"Input Size: 3 √ó 224 √ó 224\")\n",
    "print(f\"Output Classes: 102\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device, desc=\"Evaluating\"):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=desc):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(all_predictions)\n",
    "    labels = np.array(all_labels)\n",
    "    probabilities = np.array(all_probs)\n",
    "    \n",
    "    accuracy = 100.0 * (predictions == labels).sum() / len(labels)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'labels': labels,\n",
    "        'probabilities': probabilities,\n",
    "        'accuracy': accuracy,\n",
    "        'loss': avg_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVALUATING MODEL PERFORMANCE\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:19<00:00,  1.60it/s]\n",
      "Validation Set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:19<00:00,  1.68it/s]\n",
      "Test Set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 193/193 [02:04<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "üìä Training Accuracy:   99.90%\n",
      "üìä Validation Accuracy: 82.55%\n",
      "üìä Test Accuracy:       79.67%\n",
      "\n",
      "‚ö†Ô∏è Overfitting Gap (Train-Test): 20.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EVALUATE ON ALL DATASETS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATING MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate on all three sets\n",
    "train_results = evaluate_model(model, train_loader, device, \"Training Set\")\n",
    "val_results = evaluate_model(model, val_loader, device, \"Validation Set\")\n",
    "test_results = evaluate_model(model, test_loader, device, \"Test Set\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Training Accuracy:   {train_results['accuracy']:.2f}%\")\n",
    "print(f\"üìä Validation Accuracy: {val_results['accuracy']:.2f}%\")\n",
    "print(f\"üìä Test Accuracy:       {test_results['accuracy']:.2f}%\")\n",
    "print(f\"\\n‚ö†Ô∏è Overfitting Gap (Train-Test): {train_results['accuracy'] - test_results['accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ADVANCED METRICS (Test Set)\n",
      "======================================================================\n",
      "Top-1 Accuracy: 79.67%\n",
      "Top-3 Accuracy: 90.42%\n",
      "Top-5 Accuracy: 93.90%\n",
      "Top-10 Accuracy: 96.85%\n",
      "\n",
      "üéØ CONFIDENCE ANALYSIS:\n",
      "  Average Confidence: 0.797\n",
      "  Confidence when Correct: 0.878\n",
      "  Confidence when Wrong: 0.480\n",
      "  Confidence Gap: 0.398\n",
      "\n",
      "üèÜ TOP 5 BEST CLASSES:\n",
      "  1. Class 7: 100.0%\n",
      "  2. Class 9: 100.0%\n",
      "  3. Class 12: 100.0%\n",
      "  4. Class 16: 100.0%\n",
      "  5. Class 20: 100.0%\n",
      "\n",
      "üìâ TOP 5 WORST CLASSES:\n",
      "  1. Class 89: 50.0%\n",
      "  2. Class 50: 49.2%\n",
      "  3. Class 83: 48.5%\n",
      "  4. Class 10: 43.3%\n",
      "  5. Class 3: 27.8%\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ADVANCED METRICS\n",
    "# ============================================\n",
    "\n",
    "def calculate_topk_accuracy(probabilities, labels, k):\n",
    "    \"\"\"Calculate top-k accuracy\"\"\"\n",
    "    top_k_correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        top_k_preds = np.argsort(probabilities[i])[-k:][::-1]\n",
    "        if labels[i] in top_k_preds:\n",
    "            top_k_correct += 1\n",
    "    return 100.0 * top_k_correct / len(labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ADVANCED METRICS (Test Set)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Top-K accuracies\n",
    "topk_results = {}\n",
    "for k in [1, 3, 5, 10]:\n",
    "    topk_acc = calculate_topk_accuracy(\n",
    "        test_results['probabilities'], \n",
    "        test_results['labels'], \n",
    "        k\n",
    "    )\n",
    "    topk_results[k] = topk_acc\n",
    "    print(f\"Top-{k} Accuracy: {topk_acc:.2f}%\")\n",
    "\n",
    "# Confidence analysis\n",
    "predictions = test_results['predictions']\n",
    "labels = test_results['labels']\n",
    "probabilities = test_results['probabilities']\n",
    "\n",
    "confidence_scores = probabilities[np.arange(len(predictions)), predictions]\n",
    "correct_mask = predictions == labels\n",
    "\n",
    "avg_confidence = confidence_scores.mean()\n",
    "avg_conf_correct = confidence_scores[correct_mask].mean()\n",
    "avg_conf_wrong = confidence_scores[~correct_mask].mean()\n",
    "\n",
    "print(f\"\\nüéØ CONFIDENCE ANALYSIS:\")\n",
    "print(f\"  Average Confidence: {avg_confidence:.3f}\")\n",
    "print(f\"  Confidence when Correct: {avg_conf_correct:.3f}\")\n",
    "print(f\"  Confidence when Wrong: {avg_conf_wrong:.3f}\")\n",
    "print(f\"  Confidence Gap: {avg_conf_correct - avg_conf_wrong:.3f}\")\n",
    "\n",
    "# Per-class accuracy\n",
    "class_correct = {}\n",
    "class_total = {}\n",
    "for i in range(102):\n",
    "    mask = labels == i\n",
    "    if mask.sum() > 0:\n",
    "        class_total[i] = mask.sum()\n",
    "        class_correct[i] = (predictions[mask] == i).sum()\n",
    "\n",
    "class_accuracies = [(i, 100.0 * class_correct[i] / class_total[i]) \n",
    "                   for i in class_correct.keys()]\n",
    "class_accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nüèÜ TOP 5 BEST CLASSES:\")\n",
    "for i, (class_id, acc) in enumerate(class_accuracies[:5], 1):\n",
    "    print(f\"  {i}. Class {class_id}: {acc:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìâ TOP 5 WORST CLASSES:\")\n",
    "for i, (class_id, acc) in enumerate(class_accuracies[-5:], 1):\n",
    "    print(f\"  {i}. Class {class_id}: {acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMnMed/N7AatlFs4hn6mcxp",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
