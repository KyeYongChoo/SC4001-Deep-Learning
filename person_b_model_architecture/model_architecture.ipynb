{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff806086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import timm\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from baseline_model import (\n",
    "\tset_seed,\n",
    "\tget_flowers_dataloaders,\n",
    "\tcreate_baseline_model,\n",
    "\ttrain_epoch,\n",
    "\tvalidate,\n",
    ")\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "891ec9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_seed()\n",
    "train_loader, val_loader, test_loader = get_flowers_dataloaders(batch_size=32)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c75dfc",
   "metadata": {},
   "source": [
    "# Different Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39bcc0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "\tdef __init__(self, patience=10, min_delta=0):\n",
    "\t\tself.patience = patience\n",
    "\t\tself.min_delta = min_delta\n",
    "\t\tself.counter = 0\n",
    "\t\tself.min_validation_loss = np.inf\n",
    "\n",
    "\tdef __call__(self, validation_loss):\n",
    "\t\tif validation_loss < self.min_validation_loss:\n",
    "\t\t\tself.min_validation_loss = validation_loss\n",
    "\t\t\tself.counter = 0\n",
    "\t\telif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "\t\t\tself.counter += 1\n",
    "\t\t\tif self.counter >= self.patience:\n",
    "\t\t\t\treturn True\n",
    "\t\treturn False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c04740bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_performance(model_name, num_epochs = 30, batch_size = 32, lr = 0.001, weight_decay = 1e-4, scheduler_step_size = 10, scheduler_gamma = 0.1):\n",
    "\thistory = {\n",
    "\t\t'train_loss': [], 'train_acc': [],\n",
    "\t\t'val_loss': [], 'val_acc': [],\n",
    "\t}\n",
    "\n",
    "\t# Best model tracking\n",
    "\tbest_val_acc = 0.0\n",
    "\tbest_model_state = None\n",
    "\n",
    "\tearly_stop = EarlyStopper()\n",
    "\t\n",
    "\tmodel = create_baseline_model(num_classes=102, pretrained=True, model_name=model_name)\n",
    "\tmodel = model.to(device)\n",
    "\tcriterion = nn.CrossEntropyLoss().to(device)\n",
    "\toptimizer = optim.AdamW(\n",
    "\t\tmodel.parameters(),\n",
    "\t\tlr=lr,\n",
    "\t\tweight_decay=weight_decay,\n",
    "\t)\n",
    "\n",
    "\t# Learning rate scheduler\n",
    "\tscheduler = optim.lr_scheduler.StepLR(\n",
    "\t\toptimizer,\n",
    "\t\tstep_size=scheduler_step_size,\n",
    "\t\tgamma=scheduler_gamma,\n",
    "\t)\n",
    "\tassert num_epochs > 0, \"Num epochs must be more than 0!\"\n",
    "\n",
    "\tfor epoch in tqdm(range(num_epochs), f\"Training Epoch ({model_name})\", unit=\"epoch\"):\n",
    "\t\tcurrent_lr = scheduler.get_last_lr()[0]\n",
    "\t\ttrain_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\t\tval_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\t\tscheduler.step()\n",
    "\t\thistory['train_loss'].append(train_loss)\n",
    "\t\thistory['train_acc'].append(train_acc)\n",
    "\t\thistory['val_loss'].append(val_loss)\n",
    "\t\thistory['val_acc'].append(val_acc)\n",
    "\t\tif val_acc > best_val_acc:\n",
    "\t\t\tbest_val_acc = val_acc\n",
    "\t\t\tbest_model_state = model.state_dict().copy()\n",
    "\t\tif early_stop(val_loss):\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\t\ttqdm.write(\n",
    "\t\t\tf\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "\t\t\tf\"LR: {current_lr:.6f} | \"\n",
    "\t\t\tf\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\"\n",
    "\t\t\tf\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "\t\t\tf\"Test Loss: {test_loss:.4f}, Val Acc: {test_acc:.2f}%\"\n",
    "\t\t)\n",
    "\ttest_loss, test_acc = validate(model, test_loader, criterion, device, valid_or_test = \"test\")\n",
    "\thistory['test_loss'] = test_loss\n",
    "\thistory['test_acc'] = test_acc\n",
    "\tassert best_val_acc > 0.0 and best_model_state is not None, \"The model validation acc = 0!\" # my pylance can't deduce that best_model state != 0 based on best_val_acc != 0 :(\n",
    "\tmodel.load_state_dict(best_model_state)\n",
    "\tprint(f\"\\nBest Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\tnum_params = sum(p.numel() for p in model.parameters())\n",
    "\treturn model, history, num_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5924b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet models:\n",
      "['resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet32ts', 'resnet33ts', 'resnet34', 'resnet34d', 'resnet50', 'resnet50_clip', 'resnet50_clip_gap', 'resnet50_gn', 'resnet50_mlp', 'resnet50c', 'resnet50d', 'resnet50s', 'resnet50t', 'resnet50x4_clip', 'resnet50x4_clip_gap', 'resnet50x16_clip', 'resnet50x16_clip_gap', 'resnet50x64_clip', 'resnet50x64_clip_gap', 'resnet51q', 'resnet61q', 'resnet101', 'resnet101_clip', 'resnet101_clip_gap', 'resnet101c', 'resnet101d', 'resnet101s', 'resnet152', 'resnet152c', 'resnet152d', 'resnet152s', 'resnet200', 'resnet200d', 'resnetaa34d', 'resnetaa50', 'resnetaa50d', 'resnetaa101d', 'resnetblur18', 'resnetblur50', 'resnetblur50d', 'resnetblur101d', 'resnetrs50', 'resnetrs101', 'resnetrs152', 'resnetrs200', 'resnetrs270', 'resnetrs350', 'resnetrs420', 'resnetv2_18', 'resnetv2_18d', 'resnetv2_34', 'resnetv2_34d', 'resnetv2_50', 'resnetv2_50d', 'resnetv2_50d_evos', 'resnetv2_50d_frn', 'resnetv2_50d_gn', 'resnetv2_50t', 'resnetv2_50x1_bit', 'resnetv2_50x3_bit', 'resnetv2_101', 'resnetv2_101d', 'resnetv2_101x1_bit', 'resnetv2_101x3_bit', 'resnetv2_152', 'resnetv2_152d', 'resnetv2_152x2_bit', 'resnetv2_152x4_bit']\n",
      "\n",
      "ViT models:\n",
      "['vit_7b_patch16_dinov3', 'vit_base_mci_224', 'vit_base_patch8_224', 'vit_base_patch14_dinov2', 'vit_base_patch14_reg4_dinov2', 'vit_base_patch16_18x2_224', 'vit_base_patch16_224', 'vit_base_patch16_224_miil', 'vit_base_patch16_384', 'vit_base_patch16_clip_224', 'vit_base_patch16_clip_384', 'vit_base_patch16_clip_quickgelu_224', 'vit_base_patch16_dinov3', 'vit_base_patch16_dinov3_qkvb', 'vit_base_patch16_gap_224', 'vit_base_patch16_plus_240', 'vit_base_patch16_plus_clip_240', 'vit_base_patch16_reg4_gap_256', 'vit_base_patch16_rope_224', 'vit_base_patch16_rope_ape_224', 'vit_base_patch16_rope_mixed_224', 'vit_base_patch16_rope_mixed_ape_224', 'vit_base_patch16_rope_reg1_gap_256', 'vit_base_patch16_rpn_224', 'vit_base_patch16_siglip_224', 'vit_base_patch16_siglip_256', 'vit_base_patch16_siglip_384', 'vit_base_patch16_siglip_512', 'vit_base_patch16_siglip_gap_224', 'vit_base_patch16_siglip_gap_256', 'vit_base_patch16_siglip_gap_384', 'vit_base_patch16_siglip_gap_512', 'vit_base_patch16_xp_224', 'vit_base_patch32_224', 'vit_base_patch32_384', 'vit_base_patch32_clip_224', 'vit_base_patch32_clip_256', 'vit_base_patch32_clip_384', 'vit_base_patch32_clip_448', 'vit_base_patch32_clip_quickgelu_224', 'vit_base_patch32_plus_256', 'vit_base_patch32_siglip_256', 'vit_base_patch32_siglip_gap_256', 'vit_base_r26_s32_224', 'vit_base_r50_s16_224', 'vit_base_r50_s16_384', 'vit_base_resnet26d_224', 'vit_base_resnet50d_224', 'vit_betwixt_patch16_gap_256', 'vit_betwixt_patch16_reg1_gap_256', 'vit_betwixt_patch16_reg4_gap_256', 'vit_betwixt_patch16_reg4_gap_384', 'vit_betwixt_patch16_rope_reg4_gap_256', 'vit_betwixt_patch32_clip_224', 'vit_giant_patch14_224', 'vit_giant_patch14_clip_224', 'vit_giant_patch14_dinov2', 'vit_giant_patch14_reg4_dinov2', 'vit_giant_patch16_gap_224', 'vit_giantopt_patch16_siglip_256', 'vit_giantopt_patch16_siglip_384', 'vit_giantopt_patch16_siglip_gap_256', 'vit_giantopt_patch16_siglip_gap_384', 'vit_gigantic_patch14_224', 'vit_gigantic_patch14_clip_224', 'vit_gigantic_patch14_clip_378', 'vit_gigantic_patch14_clip_quickgelu_224', 'vit_huge_patch14_224', 'vit_huge_patch14_clip_224', 'vit_huge_patch14_clip_336', 'vit_huge_patch14_clip_378', 'vit_huge_patch14_clip_quickgelu_224', 'vit_huge_patch14_clip_quickgelu_378', 'vit_huge_patch14_gap_224', 'vit_huge_patch14_xp_224', 'vit_huge_patch16_gap_448', 'vit_huge_plus_patch16_dinov3', 'vit_huge_plus_patch16_dinov3_qkvb', 'vit_intern300m_patch14_448', 'vit_large_patch14_224', 'vit_large_patch14_clip_224', 'vit_large_patch14_clip_336', 'vit_large_patch14_clip_quickgelu_224', 'vit_large_patch14_clip_quickgelu_336', 'vit_large_patch14_dinov2', 'vit_large_patch14_reg4_dinov2', 'vit_large_patch14_xp_224', 'vit_large_patch16_224', 'vit_large_patch16_384', 'vit_large_patch16_dinov3', 'vit_large_patch16_dinov3_qkvb', 'vit_large_patch16_rope_224', 'vit_large_patch16_rope_ape_224', 'vit_large_patch16_rope_mixed_224', 'vit_large_patch16_rope_mixed_ape_224', 'vit_large_patch16_siglip_256', 'vit_large_patch16_siglip_384', 'vit_large_patch16_siglip_512', 'vit_large_patch16_siglip_gap_256', 'vit_large_patch16_siglip_gap_384', 'vit_large_patch16_siglip_gap_512', 'vit_large_patch32_224', 'vit_large_patch32_384', 'vit_large_r50_s32_224', 'vit_large_r50_s32_384', 'vit_little_patch16_reg1_gap_256', 'vit_little_patch16_reg4_gap_256', 'vit_medium_patch16_clip_224', 'vit_medium_patch16_gap_240', 'vit_medium_patch16_gap_256', 'vit_medium_patch16_gap_384', 'vit_medium_patch16_reg1_gap_256', 'vit_medium_patch16_reg4_gap_256', 'vit_medium_patch16_rope_reg1_gap_256', 'vit_medium_patch32_clip_224', 'vit_mediumd_patch16_reg4_gap_256', 'vit_mediumd_patch16_reg4_gap_384', 'vit_mediumd_patch16_rope_reg1_gap_256', 'vit_pe_core_base_patch16_224', 'vit_pe_core_gigantic_patch14_448', 'vit_pe_core_large_patch14_336', 'vit_pe_core_small_patch16_384', 'vit_pe_core_tiny_patch16_384', 'vit_pe_lang_gigantic_patch14_448', 'vit_pe_lang_large_patch14_448', 'vit_pe_spatial_base_patch16_512', 'vit_pe_spatial_gigantic_patch14_448', 'vit_pe_spatial_large_patch14_448', 'vit_pe_spatial_small_patch16_512', 'vit_pe_spatial_tiny_patch16_512', 'vit_pwee_patch16_reg1_gap_256', 'vit_relpos_base_patch16_224', 'vit_relpos_base_patch16_cls_224', 'vit_relpos_base_patch16_clsgap_224', 'vit_relpos_base_patch16_plus_240', 'vit_relpos_base_patch16_rpn_224', 'vit_relpos_base_patch32_plus_rpn_256', 'vit_relpos_medium_patch16_224', 'vit_relpos_medium_patch16_cls_224', 'vit_relpos_medium_patch16_rpn_224', 'vit_relpos_small_patch16_224', 'vit_relpos_small_patch16_rpn_224', 'vit_small_patch8_224', 'vit_small_patch14_dinov2', 'vit_small_patch14_reg4_dinov2', 'vit_small_patch16_18x2_224', 'vit_small_patch16_36x1_224', 'vit_small_patch16_224', 'vit_small_patch16_384', 'vit_small_patch16_dinov3', 'vit_small_patch16_dinov3_qkvb', 'vit_small_patch16_rope_224', 'vit_small_patch16_rope_ape_224', 'vit_small_patch16_rope_mixed_224', 'vit_small_patch16_rope_mixed_ape_224', 'vit_small_patch32_224', 'vit_small_patch32_384', 'vit_small_plus_patch16_dinov3', 'vit_small_plus_patch16_dinov3_qkvb', 'vit_small_r26_s32_224', 'vit_small_r26_s32_384', 'vit_small_resnet26d_224', 'vit_small_resnet50d_s16_224', 'vit_so150m2_patch16_reg1_gap_256', 'vit_so150m2_patch16_reg1_gap_384', 'vit_so150m2_patch16_reg1_gap_448', 'vit_so150m_patch16_reg4_gap_256', 'vit_so150m_patch16_reg4_gap_384', 'vit_so150m_patch16_reg4_map_256', 'vit_so400m_patch14_siglip_224', 'vit_so400m_patch14_siglip_378', 'vit_so400m_patch14_siglip_384', 'vit_so400m_patch14_siglip_gap_224', 'vit_so400m_patch14_siglip_gap_378', 'vit_so400m_patch14_siglip_gap_384', 'vit_so400m_patch14_siglip_gap_448', 'vit_so400m_patch14_siglip_gap_896', 'vit_so400m_patch16_siglip_256', 'vit_so400m_patch16_siglip_384', 'vit_so400m_patch16_siglip_512', 'vit_so400m_patch16_siglip_gap_256', 'vit_so400m_patch16_siglip_gap_384', 'vit_so400m_patch16_siglip_gap_512', 'vit_srelpos_medium_patch16_224', 'vit_srelpos_small_patch16_224', 'vit_tiny_patch16_224', 'vit_tiny_patch16_384', 'vit_tiny_r_s16_p8_224', 'vit_tiny_r_s16_p8_384', 'vit_wee_patch16_reg1_gap_256', 'vit_xsmall_patch16_clip_224', 'vitamin_base_224', 'vitamin_large2_224', 'vitamin_large2_256', 'vitamin_large2_336', 'vitamin_large2_384', 'vitamin_large_224', 'vitamin_large_256', 'vitamin_large_336', 'vitamin_large_384', 'vitamin_small_224', 'vitamin_xlarge_256', 'vitamin_xlarge_336', 'vitamin_xlarge_384']\n"
     ]
    }
   ],
   "source": [
    "# List all available ResNet models\n",
    "resnet_models = timm.list_models('resnet*')\n",
    "print(\"ResNet models:\")\n",
    "print(resnet_models)\n",
    "\n",
    "# List all available Vision Transformer (ViT) models\n",
    "vit_models = timm.list_models('vit*')\n",
    "print(\"\\nViT models:\")\n",
    "print(vit_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04381dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_model(num_classes=102, pretrained=True, model_name=\"vit_base_patch16_224\"):\n",
    "\tmodel = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "\treturn model\n",
    "\n",
    "def get_vit_performance(model_name=\"vit_b_16\", num_epochs=30, batch_size=32, lr=0.001,\n",
    "\t\t\t  weight_decay=1e-4, scheduler_step_size=10, scheduler_gamma=0.1):\n",
    "\thistory = {\n",
    "\t\t'train_loss': [], 'train_acc': [],\n",
    "\t\t'val_loss': [], 'val_acc': [],\n",
    "\t\t'test_loss': [], 'test_acc': []\n",
    "\t}\n",
    "\n",
    "\tbest_val_acc = 0.0\n",
    "\tbest_model_state = None\n",
    "\tearly_stop = EarlyStopper()\n",
    "\n",
    "\t# Create model\n",
    "\tmodel = create_vit_model(num_classes=102, pretrained=True, model_name=model_name)\n",
    "\tmodel = model.to(device)\n",
    "\n",
    "\tcriterion = nn.CrossEntropyLoss().to(device)\n",
    "\toptimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "\n",
    "\tassert num_epochs > 0, \"Num epochs must be more than 0!\"\n",
    "\n",
    "\tfor epoch in tqdm(range(num_epochs), desc=f\"Training Epoch ({model_name})\", unit=\"epoch\"):\n",
    "\t\tcurrent_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "\t\t# Train / Validate / Test\n",
    "\t\ttrain_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\t\tval_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\t\tscheduler.step()\n",
    "\n",
    "\t\t# Track history\n",
    "\t\tfor key, val in zip(['train_loss', 'train_acc', 'val_loss', 'val_acc'],\n",
    "\t\t\t\t\t\t\t[train_loss, train_acc, val_loss, val_acc]):\n",
    "\t\t\thistory[key].append(val)\n",
    "\n",
    "\t\t# Save best model\n",
    "\t\tif val_acc > best_val_acc:\n",
    "\t\t\tbest_val_acc = val_acc\n",
    "\t\t\tbest_model_state = model.state_dict().copy()\n",
    "\n",
    "\t\tif early_stop(val_loss):\n",
    "\t\t\ttqdm.write(\"Early stopping triggered.\")\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\ttqdm.write(\n",
    "\t\t\tf\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "\t\t\tf\"LR: {current_lr:.6f} | \"\n",
    "\t\t\tf\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "\t\t\tf\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "\t\t)\n",
    "\ttest_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "\thistory[\"test_loss\"] = test_loss\n",
    "\thistory[\"test_acc\"] = test_acc\n",
    "\tassert best_val_acc > 0.0 and best_model_state is not None, \"The model validation acc = 0!\" # my pylance can't deduce that best_model state != 0 based on best_val_acc != 0 :(\n",
    "\tmodel.load_state_dict(best_model_state)\n",
    "\tprint(f\"\\nBest Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\tnum_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\treturn model, history, num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c1b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e4d333adb34061b5d062795e73fd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test different depth\n",
    "resnet_depth = []\n",
    "resnet_test_acc = []\n",
    "resnet_overfit_gap = []\n",
    "resnet_histories = {}\n",
    "\n",
    "# resnet memory scales linearly with number of layers since need to keep the output tensors in memory for backprop, it looks like my GPU can handle all of them.\n",
    "# for model_name in [\n",
    "# \t\"resnet18\",\n",
    "# \t\"resnet34\",\n",
    "# \t\"resnet50\",\n",
    "# \t\"resnet101\",\n",
    "# \t\"resnet152\",\n",
    "# ]:\n",
    "# \tmodel, history, num_params = get_resnet_performance(model_name)\n",
    "# \tresnet_histories[model_name] = history\n",
    "# \tresnet_histories[model_name][\"num_params\"] = num_params \n",
    "\n",
    "# ViT scales quadratically inversely with patch size. Looks like my GPU of 7.6 GB can't handle larger models\n",
    "for model_name in [\n",
    "\t\"vit_base_patch32_224\",\n",
    "\t\"vit_base_patch16_224\",\n",
    "\t# \"vit_large_patch32_224\",\n",
    "\t# \"vit_large_patch16_224\",\n",
    "\t# \"vit_huge_patch16_224\",\n",
    "]:\n",
    "\tmodel, history, num_params = get_vit_performance(model_name)\n",
    "\tresnet_histories[model_name] = history\n",
    "\tresnet_histories[model_name][\"num_params\"] = num_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet: resnet18, resnet34, resnet50\n",
    "\n",
    "ViT: vit_base_patch16_224, vit_large_patch16_224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c510cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in test_loader:\n",
    "\tprint(images.shape)\n",
    "\tbreak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(history['train_acc'], label='Train Acc', linewidth=2)\n",
    "ax2.plot(history['val_acc'], label='Val Acc', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_vit_model(num_classes=102):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d6286",
   "metadata": {},
   "source": [
    "# Reduce Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c45721",
   "metadata": {},
   "source": [
    "# Deformable Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c1d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyterVenv15Oct2024)",
   "language": "python",
   "name": "jupytervenv15oct2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
