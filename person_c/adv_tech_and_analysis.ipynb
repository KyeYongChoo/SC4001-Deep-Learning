{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a6c065",
   "metadata": {},
   "source": [
    "# SC4001 Deep Learning - Group Project\n",
    "## Oxford Flowers 102 Recognition\n",
    "\n",
    "### Role: Person C - Advanced Techniques & Analysis\n",
    "\n",
    "This notebook contains:\n",
    "1. Few-Shot Learning experiments (1, 5, 10 samples per class)\n",
    "2. Advanced Techniques (MixUp, Triplet Loss)\n",
    "3. Performance comparison and reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb948cd",
   "metadata": {},
   "source": [
    "## Part 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df486967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found at: ../data/flowers-102\n",
      "Loading Flowers102 dataset...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbaseline_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m \tset_seed,\n\u001b[32m     20\u001b[39m \tget_flowers_dataloaders,\n\u001b[32m     21\u001b[39m \ttrain_epoch,\n\u001b[32m     22\u001b[39m \tvalidate,\n\u001b[32m     23\u001b[39m \tcreate_model_with_timeout\n\u001b[32m     24\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SC4001-Deep-Learning/person_c_advanced_techniques_and_analysis/baseline_model.py:109\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading Flowers102 dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Load test dataset with transforms\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m test_dataset = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFlowers102\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m\t\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Changed to False\u001b[39;49;00m\n\u001b[32m    113\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Load without transforms for exploration\u001b[39;00m\n\u001b[32m    117\u001b[39m train_dataset_raw = datasets.Flowers102(\n\u001b[32m    118\u001b[39m \troot=data_root, \n\u001b[32m    119\u001b[39m \tsplit=\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    120\u001b[39m \tdownload=\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Already False\u001b[39;00m\n\u001b[32m    121\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SC4001-Deep-Learning/venv/lib/python3.14/site-packages/torchvision/datasets/flowers102.py:64\u001b[39m, in \u001b[36mFlowers102.__init__\u001b[39m\u001b[34m(self, root, split, transform, target_transform, download, loader)\u001b[39m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.download()\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loadmat\n\u001b[32m     68\u001b[39m set_ids = loadmat(\u001b[38;5;28mself\u001b[39m._base_folder / \u001b[38;5;28mself\u001b[39m._file_dict[\u001b[33m\"\u001b[39m\u001b[33msetid\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m], squeeze_me=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from baseline_model import (\n",
    "\tset_seed,\n",
    "\tget_flowers_dataloaders,\n",
    "\ttrain_epoch,\n",
    "\tvalidate,\n",
    "\tcreate_model_with_timeout\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e2b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not import from baseline_model.py\n",
      "Please ensure 'baseline_model.py' is in the same folder as this notebook.\n",
      "\n",
      "Person C - Advanced Techniques Module Loaded\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'set_seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Setup\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[32m     28\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mset_seed\u001b[49m(\u001b[32m42\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# --- Define Data Path ---\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# This assumes the 'data' folder is one level up from the notebook directory\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Adjust if your structure is different (e.g., 'data/')\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'set_seed' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from baseline_model import (\n",
    "        set_seed,\n",
    "        get_data_transforms,\n",
    "        create_baseline_model,\n",
    "        train_epoch,\n",
    "        validate,\n",
    "        create_model_with_timeout,\n",
    "        get_flowers_dataloaders \n",
    "    )\n",
    "    print(\"Successfully imported from baseline_model.py\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: Could not import from baseline_model.py\")\n",
    "    print(\"Please ensure 'baseline_model.py' is in the same folder as this notebook.\")\n",
    "\n",
    "print(\"\\nPerson C - Advanced Techniques Module Loaded\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "set_seed(42)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "DATA_PATH = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7beb5",
   "metadata": {},
   "source": [
    "## Part 2: Few-Shot Learning\n",
    "\n",
    "Code for creating k-shot datasets and training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f60261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotDataset:\n",
    "    \"\"\"Create few-shot subsets of the training data\"\"\"\n",
    "    \n",
    "    def __init__(self, full_dataset, k_shot, num_classes=102, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            full_dataset: Original training dataset (MUST be loaded with transform=None)\n",
    "            k_shot: Number of samples per class (1, 5, or 10)\n",
    "            num_classes: Total number of classes\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.full_dataset = full_dataset\n",
    "        self.k_shot = k_shot\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Group indices by class\n",
    "        class_indices = defaultdict(list)\n",
    "        \n",
    "        # Use _labels attribute from Flowers102 dataset for efficiency\n",
    "        try:\n",
    "            labels = self.full_dataset._labels\n",
    "            for idx, label in enumerate(labels):\n",
    "                class_indices[label].append(idx)\n",
    "        except AttributeError:\n",
    "            print(\"Warning: Could not use ._labels, iterating dataset slowly...\")\n",
    "            for idx in range(len(full_dataset)):\n",
    "                _, label = full_dataset[idx] \n",
    "                class_indices[label].append(idx)\n",
    "        \n",
    "        # Sample k images per class\n",
    "        self.selected_indices = []\n",
    "        for class_id in range(num_classes):\n",
    "            if len(class_indices[class_id]) >= k_shot:\n",
    "                sampled = random.sample(class_indices[class_id], k_shot)\n",
    "                self.selected_indices.extend(sampled)\n",
    "            else:\n",
    "                # Safeguard if a class has < k_shot samples (shouldn't happen for 1, 5, 10)\n",
    "                print(f\"Warning: Class {class_id} has fewer than {k_shot} samples. Using all.\")\n",
    "                self.selected_indices.extend(class_indices[class_id])\n",
    "        \n",
    "        print(f\"\\nFew-Shot Dataset Created:\")\n",
    "        print(f\"  K-shot: {k_shot}\")\n",
    "        print(f\"  Total samples: {len(self.selected_indices)}\")\n",
    "        print(f\"  Expected: {num_classes * k_shot}\")\n",
    "    \n",
    "    def get_subset(self, target_dataset):\n",
    "        \"\"\"Return PyTorch Subset *from the target_dataset* using selected indices\"\"\"\n",
    "        return Subset(target_dataset, self.selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b844d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fewshot_dataloaders(k_shot, batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create dataloaders for few-shot learning\n",
    "    \"\"\"\n",
    "    train_transform, test_transform = get_data_transforms()\n",
    "    \n",
    "    # Load a RAW version of the training set (transform=None) \n",
    "    # This is ONLY for sampling indices based on labels.\n",
    "    train_dataset_raw = datasets.Flowers102(\n",
    "        root=DATA_PATH,\n",
    "        split='train',\n",
    "        download=True,\n",
    "        transform=None\n",
    "    )\n",
    "    \n",
    "    # Load the TRANSFORMED version of the training set.\n",
    "    # This is the dataset we will actually train on.\n",
    "    train_dataset_full = datasets.Flowers102(\n",
    "        root=DATA_PATH,\n",
    "        split='train',\n",
    "        download=False, # Already downloaded\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    # Create few-shot sampler using the RAW dataset to get indices\n",
    "    fewshot_sampler = FewShotDataset(train_dataset_raw, k_shot=k_shot)\n",
    "    # Create the final Subset using the TRANSFORMED dataset\n",
    "    train_subset = fewshot_sampler.get_subset(train_dataset_full)\n",
    "    \n",
    "    # Validation and test remain the same\n",
    "    val_dataset = datasets.Flowers102(\n",
    "        root=DATA_PATH,\n",
    "        split='val',\n",
    "        download=True,\n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = datasets.Flowers102(\n",
    "        root=DATA_PATH,\n",
    "        split='test',\n",
    "        download=True,\n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    pin_mem = True if torch.cuda.is_available() else False\n",
    "    num_workers = 2 if os.name == 'nt' else num_workers\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_mem\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_mem\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_mem\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8efb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fewshot_model(k_shot, model_name='resnet18', num_epochs=50, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train model with few-shot learning\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {k_shot}-Shot Learning Model (Arch: {model_name})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create few-shot dataloaders\n",
    "    train_loader, val_loader, test_loader = create_fewshot_dataloaders(\n",
    "        k_shot=k_shot,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    if 'vit' in model_name:\n",
    "        model = create_model_with_timeout(model_name, num_classes=102, device=device)\n",
    "    else:\n",
    "        model = create_baseline_model(num_classes=102, pretrained=True, model_name=model_name)\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "    \n",
    "    # Training loop\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"  ✓ New best! Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "        print(f\"  Val: Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    # Load best model and evaluate on test\n",
    "    model.load_state_dict(best_model_state)\n",
    "    test_loss, test_acc = validate(model, test_loader, criterion, device, valid_or_test=\"test\")\n",
    "    \n",
    "    history['best_val_acc'] = best_val_acc\n",
    "    history['test_loss'] = test_loss\n",
    "    history['test_acc'] = test_acc\n",
    "    history['k_shot'] = k_shot\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{k_shot}-Shot Results ({model_name}):\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    print(f\"  Test Acc: {test_acc:.2f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57b858",
   "metadata": {},
   "source": [
    "## Part 3: MixUp Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de42ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    Apply MixUp augmentation\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"MixUp loss function\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "def train_epoch_mixup(model, dataloader, criterion, optimizer, device, alpha=1.0):\n",
    "    \"\"\"Training epoch with MixUp augmentation\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training (MixUp)')\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Apply MixUp\n",
    "        mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, alpha, device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixed_images)\n",
    "        loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (lam * predicted.eq(labels_a).sum().item() + \n",
    "                    (1 - lam) * predicted.eq(labels_b).sum().item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss/(batch_idx+1),\n",
    "            'acc': 100.*correct/total\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(dataloader), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_mixup(model_name='resnet18', num_epochs=30, alpha=1.0, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train model with MixUp augmentation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with MixUp (alpha={alpha}, Arch: {model_name})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Import baseline dataloader (this uses the 10-shot train set)\n",
    "    train_loader, val_loader, test_loader = get_flowers_dataloaders(\n",
    "        batch_size=32, data_path=DATA_PATH\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    if 'vit' in model_name:\n",
    "        model = create_model_with_timeout(model_name, num_classes=102, device=device)\n",
    "    else:\n",
    "        model = create_baseline_model(num_classes=102, pretrained=True, model_name=model_name)\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    # Training loop\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train with MixUp\n",
    "        train_loss, train_acc = train_epoch_mixup(\n",
    "            model, train_loader, criterion, optimizer, device, alpha=alpha\n",
    "        )\n",
    "        \n",
    "        # Validate (no MixUp)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"  ✓ New best! Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "        print(f\"  Val: Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.load_state_dict(best_model_state)\n",
    "    test_loss, test_acc = validate(model, test_loader, criterion, device, valid_or_test=\"test\")\n",
    "    \n",
    "    history['best_val_acc'] = best_val_acc\n",
    "    history['test_loss'] = test_loss\n",
    "    history['test_acc'] = test_acc\n",
    "    history['alpha'] = alpha\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MixUp Results (alpha={alpha}, {model_name}):\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    print(f\"  Test Acc: {test_acc:.2f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173efd9f",
   "metadata": {},
   "source": [
    "## Part 4: Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59724588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet Loss for metric learning\"\"\"\n",
    "    \n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = F.pairwise_distance(anchor, positive, 2)\n",
    "        distance_negative = F.pairwise_distance(anchor, negative, 2)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    \"\"\"Dataset that returns triplets (anchor, positive, negative)\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        \n",
    "        # Group indices by class\n",
    "        self.class_indices = defaultdict(list)\n",
    "        try:\n",
    "            labels = self.base_dataset._labels\n",
    "            for idx, label in enumerate(labels):\n",
    "                self.class_indices[label].append(idx)\n",
    "        except AttributeError:\n",
    "            print(\"Warning: Iterating slowly for TripletDataset...\")\n",
    "            for idx in range(len(base_dataset)):\n",
    "                _, label = base_dataset[idx]\n",
    "                self.class_indices[label].append(idx)\n",
    "                self.classes = list(self.class_indices.keys())\n",
    "                self.class_count = len(self.classes)\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get anchor\n",
    "        anchor_img, anchor_label = self.base_dataset[idx]\n",
    "        \n",
    "        # Get positive (same class)\n",
    "        positive_indices = self.class_indices[anchor_label]\n",
    "        positive_idx = idx\n",
    "        if len(positive_indices) > 1:\n",
    "            while positive_idx == idx:\n",
    "                positive_idx = random.choice(positive_indices)\n",
    "        else:\n",
    "            positive_idx = idx\n",
    "            \n",
    "        positive_img, _ = self.base_dataset[positive_idx]\n",
    "        \n",
    "        # Get negative (different class)\n",
    "        negative_label = anchor_label\n",
    "        while negative_label == anchor_label:\n",
    "            negative_label = random.choice(self.classes)\n",
    "            \n",
    "        negative_idx = random.choice(self.class_indices[negative_label])\n",
    "        negative_img, _ = self.base_dataset[negative_idx]\n",
    "        \n",
    "        return anchor_img, positive_img, negative_img, anchor_label                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09536bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletModelWrapper(nn.Module):\n",
    "    \"\"\"Wraps a base model for triplet loss AND classification\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, embedding_dim=128, num_classes=102):\n",
    "        super(TripletModelWrapper, self).__init__()\n",
    "        \n",
    "        # Get feature dimension from the base_model's fc layer\n",
    "        try:\n",
    "            num_features = base_model.fc.in_features\n",
    "            # Overwrite the fc layer to be an embedding layer\n",
    "            base_model.fc = nn.Linear(num_features, embedding_dim)\n",
    "        except AttributeError:\n",
    "            # Handle models like ViT which use 'head'\n",
    "            num_features = base_model.head.in_features\n",
    "            base_model.head = nn.Linear(num_features, embedding_dim)\n",
    "            \n",
    "        self.base_model = base_model\n",
    "        \n",
    "        # Add a *new* classification head after the embedding layer\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        # Get the embedding from the modified base model\n",
    "        embedding = self.base_model(x)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        \n",
    "        # Pass the embedding to the new classifier\n",
    "        logits = self.classifier(embedding)\n",
    "        return logits, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b601c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_triplet(model, dataloader, criterion_triplet, criterion_cls, \n",
    "                        optimizer, device, lambda_triplet=0.5):\n",
    "    \"\"\"Training epoch with combined triplet and classification loss\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_triplet_loss = 0.0\n",
    "    running_cls_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training (Triplet)')\n",
    "    \n",
    "    for batch_idx, (anchor, positive, negative, labels) in enumerate(pbar):\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get outputs for all three images\n",
    "        anchor_out, anchor_emb = model(anchor)\n",
    "        positive_out, positive_emb = model(positive)\n",
    "        negative_out, negative_emb = model(negative)\n",
    "        \n",
    "        # 1. Compute Triplet loss\n",
    "        triplet_loss = criterion_triplet(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        # 2. Compute Classification loss (on the anchor)\n",
    "        cls_loss = criterion_cls(anchor_out, labels)\n",
    "        \n",
    "        # 3. Combined loss\n",
    "        loss = lambda_triplet * triplet_loss + (1 - lambda_triplet) * cls_loss\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        running_triplet_loss += triplet_loss.item()\n",
    "        running_cls_loss += cls_loss.item()\n",
    "        \n",
    "        _, predicted = anchor_out.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss/(batch_idx+1),\n",
    "            'triplet': running_triplet_loss/(batch_idx+1),\n",
    "            'cls': running_cls_loss/(batch_idx+1),\n",
    "            'acc': 100.*correct/total\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(dataloader), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceda95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_triplet_loss(model_name='resnet18', num_epochs=30, margin=1.0, \n",
    "                            lambda_triplet=0.5, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train model with triplet loss\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with Triplet Loss (margin={margin}, lambda={lambda_triplet}, Arch={model_name})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load datasets\n",
    "    train_transform, test_transform = get_data_transforms()\n",
    "    \n",
    "    # Note: TripletDataset needs the base dataset *with transforms*\n",
    "    train_dataset = datasets.Flowers102(\n",
    "        root=DATA_PATH,\n",
    "        split='train',\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = datasets.Flowers102(\n",
    "        root=DATA_PATH,\n",
    "        split='val',\n",
    "        download=True,\n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = datasets.Flowers102(\n",
    "        root=DATA_PATH,\n",
    "        split='test',\n",
    "        download=True,\n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    # Create triplet dataset for training\n",
    "    triplet_train_dataset = TripletDataset(train_dataset)\n",
    "    \n",
    "    num_workers = 2 if os.name == 'nt' else 4\n",
    "    pin_mem = True if torch.cuda.is_available() else False\n",
    "    \n",
    "    train_loader = DataLoader(triplet_train_dataset, batch_size=32, shuffle=True, num_workers=num_workers, pin_memory=pin_mem)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=pin_mem)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=pin_mem)\n",
    "    \n",
    "    # Create base model (e.g., ResNet18)\n",
    "    if 'vit' in model_name:\n",
    "        base_model = create_model_with_timeout(model_name, num_classes=102, device=device, pretrained=True)\n",
    "    else:\n",
    "        base_model = create_baseline_model(num_classes=102, pretrained=True, model_name=model_name)\n",
    "    \n",
    "    # Wrap in triplet model\n",
    "    model = TripletModelWrapper(base_model, embedding_dim=128, num_classes=102)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion_triplet = TripletLoss(margin=margin)\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    # Training loop\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train with triplet loss\n",
    "        train_loss, train_acc = train_epoch_triplet(\n",
    "            model, train_loader, criterion_triplet, criterion_cls,\n",
    "            optimizer, device, lambda_triplet=lambda_triplet\n",
    "        )\n",
    "        \n",
    "        # Validate (classification only)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion_cls, device)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"  ✓ New best! Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "        print(f\"  Val: Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.load_state_dict(best_model_state)\n",
    "    test_loss, test_acc = validate(model, test_loader, criterion_cls, device, valid_or_test=\"test\")\n",
    "    \n",
    "    history['best_val_acc'] = best_val_acc\n",
    "    history['test_loss'] = test_loss\n",
    "    history['test_acc'] = test_acc\n",
    "    history['margin'] = margin\n",
    "    history['lambda_triplet'] = lambda_triplet\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Triplet Loss Results ({model_name}):\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    print(f\"  Test Acc: {test_acc:.2f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b74452",
   "metadata": {},
   "source": [
    "## Part 5: Experiment Runner\n",
    "\n",
    "This master function runs all experiments for Person C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_experiments(quick_test=False):\n",
    "    \"\"\"\n",
    "    Run all Person C experiments\n",
    "    \n",
    "    Args:\n",
    "        quick_test: If True, run with reduced epochs for testing\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary containing all experiment results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Set epochs based on mode\n",
    "    epochs_fewshot = 50 if not quick_test else 3 # 50 epochs for few-shot\n",
    "    epochs_standard = 30 if not quick_test else 3 # 30 epochs for others\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERSON C - ADVANCED TECHNIQUES EXPERIMENTS\")\n",
    "    print(f\"QUICK TEST MODE: {quick_test}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define base model for comparison\n",
    "    # We use resnet18 to compare against Person A's baseline\n",
    "    # We can also run this with 'vit_base_patch32_224' to compare against Person B\n",
    "    BASE_MODEL_NAME = 'resnet18'\n",
    "    \n",
    "    #EXPERIMENT 1: Few-Shot Learning\n",
    "    print(\"\\n\\n EXPERIMENT 1: Few-Shot Learning \\n\")\n",
    "    \n",
    "    fewshot_results = {}\n",
    "    for k_shot in [1, 5, 10]:\n",
    "        try:\n",
    "            model, history = train_fewshot_model(\n",
    "                k_shot=k_shot,\n",
    "                model_name=BASE_MODEL_NAME,\n",
    "                num_epochs=epochs_fewshot,\n",
    "                lr=0.001\n",
    "            )\n",
    "            fewshot_results[f'{k_shot}_shot'] = history\n",
    "            \n",
    "            # Save model\n",
    "            save_dir = f'results/person_c/fewshot_{k_shot}shot_{BASE_MODEL_NAME}'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'{save_dir}/model.pth')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in {k_shot}-shot learning: {e}\")\n",
    "            if \"baseline_model\" in str(e):\n",
    "                print(\"*** HINT: 'baseline_model.py' not found. Stopping. ***\")\n",
    "                break \n",
    "    \n",
    "    results['fewshot'] = fewshot_results\n",
    "    \n",
    "    #EXPERIMENT 2: MixUp Augmentation\n",
    "    print(\"\\n\\n EXPERIMENT 2: MixUp Augmentation \\n\")\n",
    "    \n",
    "    mixup_results = {}\n",
    "    # Run only on alpha=1.0 for the report, as specified in handout\n",
    "    # You can add [0.2, 0.5, 1.0] to test more\n",
    "    for alpha in [1.0]: \n",
    "        try:\n",
    "            model, history = train_with_mixup(\n",
    "                model_name=BASE_MODEL_NAME,\n",
    "                num_epochs=epochs_standard,\n",
    "                alpha=alpha,\n",
    "                lr=0.001\n",
    "            )\n",
    "            mixup_results[f'alpha_{alpha}'] = history\n",
    "            \n",
    "            # Save model\n",
    "            save_dir = f'results/person_c/mixup_alpha{alpha}_{BASE_MODEL_NAME}'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'{save_dir}/model.pth')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in MixUp (alpha={alpha}): {e}\")\n",
    "            if \"baseline_model\" in str(e):\n",
    "                print(\"*** HINT: 'baseline_model.py' not found. Stopping. ***\")\n",
    "                break\n",
    "    \n",
    "    results['mixup'] = mixup_results\n",
    "    \n",
    "    #EXPERIMENT 3: Triplet Loss\n",
    "    print(\"\\n\\n EXPERIMENT 3: Triplet Loss \\n\")\n",
    "    \n",
    "    try:\n",
    "        model, history = train_with_triplet_loss(\n",
    "            model_name=BASE_MODEL_NAME,\n",
    "            num_epochs=epochs_standard,\n",
    "            margin=1.0,\n",
    "            lambda_triplet=0.5,\n",
    "            lr=0.001\n",
    "        )\n",
    "        results['triplet'] = history\n",
    "        \n",
    "        # Save model\n",
    "        save_dir = f'results/person_c/triplet_{BASE_MODEL_NAME}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), f'{save_dir}/model.pth')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in Triplet Loss: {e}\")\n",
    "        if \"baseline_model\" in str(e):\n",
    "            print(\"*** HINT: 'baseline_model.py' not found. Stopping. ***\")\n",
    "            \n",
    "    # Save all results\n",
    "    os.makedirs('results/person_c', exist_ok=True)\n",
    "    with open(f'results/person_c/all_results_{BASE_MODEL_NAME}.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ALL EXPERIMENTS COMPLETED!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ed6d6",
   "metadata": {},
   "source": [
    "## Part 6: Visualization and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fewshot_comparison(fewshot_results, baseline_acc=None):\n",
    "    \"\"\"Plot few-shot learning comparison\"\"\"\n",
    "    if not fewshot_results:\n",
    "        print(\"No few-shot results to plot.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Validation accuracy\n",
    "    for k_name, history in fewshot_results.items():\n",
    "        val_acc = history['val_acc']\n",
    "        axes[0].plot(val_acc, label=k_name, linewidth=2)\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "    axes[0].set_title('Few-Shot Learning: Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test accuracy bar chart\n",
    "    k_shots = []\n",
    "    test_accs = []\n",
    "    for k_name, history in fewshot_results.items():\n",
    "        k_shots.append(k_name)\n",
    "        test_accs.append(history['test_acc'])\n",
    "    \n",
    "    axes[1].bar(k_shots, test_accs, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "    if baseline_acc:\n",
    "        axes[1].axhline(y=baseline_acc, color='r', linestyle='--', label=f'10-shot Baseline ({baseline_acc:.2f}%)')\n",
    "        axes[1].legend()\n",
    "        \n",
    "    axes[1].set_xlabel('K-Shot', fontsize=12)\n",
    "    axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    axes[1].set_title('Few-Shot Learning: Test Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, v in enumerate(test_accs):\n",
    "        axes[1].text(i, v + 1, f'{v:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/person_c/fewshot_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd55022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mixup_comparison(mixup_results, baseline_acc=None):\n",
    "    \"\"\"Plot MixUp augmentation comparison\"\"\"\n",
    "    if not mixup_results:\n",
    "        print(\"No MixUp results to plot.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Validation accuracy\n",
    "    for alpha_name, history in mixup_results.items():\n",
    "        val_acc = history['val_acc']\n",
    "        axes[0].plot(val_acc, label=alpha_name, linewidth=2)\n",
    "    \n",
    "\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "    axes[0].set_title('MixUp Augmentation: Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test accuracy bar chart\n",
    "    alphas = []\n",
    "    test_accs = []\n",
    "    for alpha_name, history in mixup_results.items():\n",
    "        alphas.append(alpha_name.replace('alpha_', 'α='))\n",
    "        test_accs.append(history['test_acc'])\n",
    "    \n",
    "    axes[1].bar(alphas, test_accs, color=['#9467bd', '#8c564b', '#e377c2'])\n",
    "    if baseline_acc:\n",
    "        axes[1].axhline(y=baseline_acc, color='r', linestyle='--', label=f'Baseline ({baseline_acc:.2f}%)')\n",
    "        axes[1].legend()\n",
    "        \n",
    "    axes[1].set_xlabel('MixUp Alpha', fontsize=12)\n",
    "    axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    axes[1].set_title('MixUp Augmentation: Test Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, v in enumerate(test_accs):\n",
    "        axes[1].text(i, v + 1, f'{v:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/person_c/mixup_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_table(results, baseline_acc=79.67, person_b_best_acc=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive comparison table\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE RESULTS COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Baseline comparison\n",
    "    print(\"\\n### Baseline (Person A) ###\")\n",
    "    print(f\"ResNet18 (10-shot Training): {baseline_acc:.2f}%\")\n",
    "    \n",
    "    if person_b_best_acc:\n",
    "        print(f\"\\n### Best Architecture (Person B) ###\")\n",
    "        print(f\"Best Model (ViT): {person_b_best_acc:.2f}% (For reference)\")\n",
    "    \n",
    "    # Few-shot results\n",
    "    print(\"\\n### Few-Shot Learning (Person C, ResNet18) ###\")\n",
    "    print(f\"{'Method':<30} {'Val Acc':<15} {'Test Acc':<15} {'vs Baseline':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    if 'fewshot' in results and results['fewshot']:\n",
    "        for k_name, history in results['fewshot'].items():\n",
    "            val_acc = history['best_val_acc']\n",
    "            test_acc = history['test_acc']\n",
    "            diff = test_acc - baseline_acc\n",
    "            sign = '+' if diff > 0 else ''\n",
    "            print(f\"{k_name:<30} {val_acc:<15.2f} {test_acc:<15.2f} {sign}{diff:.2f}%\")\n",
    "    else:\n",
    "        print(\"No few-shot results to display.\")\n",
    "    \n",
    "    # MixUp results\n",
    "    print(\"\\n### MixUp Augmentation (Person C, ResNet18) ###\")\n",
    "    print(f\"{'Method':<30} {'Val Acc':<15} {'Test Acc':<15} {'vs Baseline':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    if 'mixup' in results and results['mixup']:\n",
    "        for alpha_name, history in results['mixup'].items():\n",
    "            val_acc = history['best_val_acc']\n",
    "            test_acc = history['test_acc']\n",
    "            diff = test_acc - baseline_acc\n",
    "            sign = '+' if diff > 0 else ''\n",
    "            print(f\"MixUp {alpha_name:<22} {val_acc:<15.2f} {test_acc:<15.2f} {sign}{diff:.2f}%\")\n",
    "    else:\n",
    "        print(\"No MixUp results to display.\")\n",
    "        \n",
    "    # Triplet loss results\n",
    "    print(\"\\n### Triplet Loss (Person C, ResNet18) ###\")\n",
    "    print(f\"{'Method':<30} {'Val Acc':<15} {'Test Acc':<15} {'vs Baseline':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    if 'triplet' in results and results['triplet']:\n",
    "        history = results['triplet']\n",
    "        val_acc = history['best_val_acc']\n",
    "        test_acc = history['test_acc']\n",
    "        diff = test_acc - baseline_acc\n",
    "        sign = '+' if diff > 0 else ''\n",
    "        # FIX 2: Corrected the f-string typo \n",
    "        method_name = f\"Triplet Loss (λ={history['lambda_triplet']}, m={history['margin']})\"\n",
    "        print(f\"{method_name:<30} {val_acc:<15.2f} {test_acc:<15.2f} {sign}{diff:.2f}%\")\n",
    "    else:\n",
    "        print(\"No Triplet Loss results to display.\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_techniques_comparison(results, baseline_acc=79.67):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization comparing all techniques\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Person C: Advanced Techniques Analysis (on ResNet18)', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # SUBPLOT 1: Test Accuracy Comparison Bar Chart \n",
    "    techniques = ['Baseline\\n(10-shot)']\n",
    "    accuracies = [baseline_acc]\n",
    "    colors = ['#1f77b4']\n",
    "    \n",
    "    # Add few-shot results\n",
    "    if 'fewshot' in results and results['fewshot']:\n",
    "        # Find 10-shot to avoid double-plotting\n",
    "        baseline_10_shot = results['fewshot'].get('10_shot', {'test_acc': baseline_acc})\n",
    "        accuracies[0] = baseline_10_shot['test_acc'] \n",
    "        \n",
    "        for k_name, history in sorted(results['fewshot'].items()):\n",
    "            if k_name == '10_shot': continue \n",
    "            techniques.append(f'Few-Shot\\n{k_name}')\n",
    "            accuracies.append(history['test_acc'])\n",
    "            colors.append('#ff7f0e')\n",
    "    \n",
    "    # Add MixUp results\n",
    "    if 'mixup' in results and results['mixup']:\n",
    "        for alpha_name, history in sorted(results['mixup'].items()):\n",
    "            alpha_val = alpha_name.split('_')[1]\n",
    "            techniques.append(f'MixUp\\nα={alpha_val}')\n",
    "            accuracies.append(history['test_acc'])\n",
    "            colors.append('#2ca02c')\n",
    "    \n",
    "    # Add Triplet loss\n",
    "    if 'triplet' in results and results['triplet']:\n",
    "        techniques.append('Triplet\\nLoss')\n",
    "        accuracies.append(results['triplet']['test_acc'])\n",
    "        colors.append('#d62728')\n",
    "    \n",
    "    bars = axes[0, 0].bar(range(len(techniques)), accuracies, color=colors)\n",
    "    axes[0, 0].set_xticks(range(len(techniques)))\n",
    "    axes[0, 0].set_xticklabels(techniques, rotation=45, ha='right')\n",
    "    axes[0, 0].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    axes[0, 0].set_title('Test Accuracy Comparison: All Techniques', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].axhline(y=baseline_acc, color='r', linestyle='--', label=f'Baseline ({baseline_acc:.2f}%)', alpha=0.7)\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                        f'{acc:.2f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # SUBPLOT 2: Few-Shot Learning Detailed \n",
    "    if 'fewshot' in results and results['fewshot']:\n",
    "        for k_name, history in results['fewshot'].items():\n",
    "            axes[0, 1].plot(history['val_acc'], label=k_name, linewidth=2, marker='o', markersize=4, alpha=0.8)\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "        axes[0, 1].set_title('Few-Shot Learning: Training Progress', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'Few-Shot results not available.', ha='center', va='center')\n",
    "\n",
    "    # SUBPLOT 3: MixUp Alpha Comparison\n",
    "    if 'mixup' in results and results['mixup']:\n",
    "        for alpha_name, history in results['mixup'].items():\n",
    "            alpha_val = alpha_name.split('_')[1]\n",
    "            axes[1, 0].plot(history['val_acc'], label=f'α={alpha_val}', linewidth=2, marker='s', markersize=4, alpha=0.8)\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "        axes[1, 0].set_title('MixUp Augmentation: Training Progress', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'MixUp results not available.', ha='center', va='center')\n",
    "\n",
    "    # SUBPLOT 4: Improvement vs Baseline \n",
    "    techniques_impr = []\n",
    "    improvements = []\n",
    "    colors_impr = []\n",
    "    \n",
    "    # Few-shot improvements\n",
    "    if 'fewshot' in results and results['fewshot']:\n",
    "        for k_name, history in sorted(results['fewshot'].items()):\n",
    "            techniques_impr.append(f'{k_name}')\n",
    "            improvements.append(history['test_acc'] - baseline_acc)\n",
    "            colors_impr.append('#ff7f0e')\n",
    "    \n",
    "    # MixUp improvements\n",
    "    if 'mixup' in results and results['mixup']:\n",
    "        for alpha_name, history in sorted(results['mixup'].items()):\n",
    "            alpha_val = alpha_name.split('_')[1]\n",
    "            techniques_impr.append(f'MixUp α={alpha_val}')\n",
    "            improvements.append(history['test_acc'] - baseline_acc)\n",
    "            colors_impr.append('#2ca02c')\n",
    "    \n",
    "    # Triplet improvement\n",
    "    if 'triplet' in results and results['triplet']:\n",
    "        techniques_impr.append('Triplet Loss')\n",
    "        improvements.append(results['triplet']['test_acc'] - baseline_acc)\n",
    "        colors_impr.append('#d62728')\n",
    "    \n",
    "    if improvements:\n",
    "        bars = axes[1, 1].barh(range(len(techniques_impr)), improvements, color=colors_impr)\n",
    "        axes[1, 1].set_yticks(range(len(techniques_impr)))\n",
    "        axes[1, 1].set_yticklabels(techniques_impr)\n",
    "        axes[1, 1].set_xlabel('Improvement vs Baseline (%)', fontsize=12)\n",
    "        axes[1, 1].set_title('Performance Gain/Loss vs Baseline', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].axvline(x=0, color='k', linestyle='-', linewidth=0.8)\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add values on bars\n",
    "        for i, (bar, impr) in enumerate(zip(bars, improvements)):\n",
    "            width = bar.get_width()\n",
    "            sign = '+' if impr >= 0 else ''\n",
    "            ha = 'left' if width >= 0 else 'right'\n",
    "            x_pos = width + (0.2 if width >= 0 else -0.2)\n",
    "            \n",
    "            axes[1, 1].text(x_pos, bar.get_y() + bar.get_height()/2.,\n",
    "                            f'{sign}{impr:.2f}%', ha=ha, \n",
    "                            va='center', fontsize=9, fontweight='bold')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No results to plot.', ha='center', va='center')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig('results/person_c/comprehensive_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986da2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis_report(results, baseline_acc=79.67):\n",
    "    \"\"\"\n",
    "    Generate detailed analysis report\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    \n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"PERSON C - ADVANCED TECHNIQUES & ANALYSIS\")\n",
    "    report.append(\"DETAILED PERFORMANCE REPORT\")\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Few-Shot Learning Analysis \n",
    "    report.append(\"### 1. FEW-SHOT LEARNING ANALYSIS (ResNet18) ###\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"Objective: Evaluate model performance with limited training data.\")\n",
    "    report.append(f\"Baseline (10-shot training): {baseline_acc:.2f}%\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    if 'fewshot' in results and results['fewshot']:\n",
    "        report.append(\"Results:\")\n",
    "        for k_name, history in sorted(results['fewshot'].items()):\n",
    "            k_val = history['k_shot']\n",
    "            test_acc = history['test_acc']\n",
    "            val_acc = history['best_val_acc']\n",
    "            diff = test_acc - baseline_acc\n",
    "            sign = '+' if diff > 0 else ''\n",
    "            \n",
    "            report.append(f\"  {k_name}:\")\n",
    "            report.append(f\"    Training samples: {k_val * 102} ({k_val} per class)\")\n",
    "            report.append(f\"    Best val accuracy: {val_acc:.2f}%\")\n",
    "            report.append(f\"    Test accuracy: {test_acc:.2f}%\")\n",
    "            report.append(f\"    vs Baseline: {sign}{diff:.2f}%\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        report.append(\"Key Findings:\")\n",
    "        report.append(\"  • As expected, test accuracy drops significantly as training samples decrease.\")\n",
    "        report.append(\"  • The 10-shot model is the baseline. The 5-shot and 1-shot results show\")\n",
    "        report.append(\"    a clear drop, highlighting the data-hungry nature of deep learning.\")\n",
    "        report.append(\"  • Even with only 1 sample per class, the model achieves non-trivial performance,\")\n",
    "        report.append(\"    which shows the power of transfer learning from ImageNet.\")\n",
    "        report.append(\"\")\n",
    "    else:\n",
    "        report.append(\"No few-shot results to analyze.\\n\")\n",
    "\n",
    "    # MixUp Analysis \n",
    "    report.append(\"### 2. MIXUP AUGMENTATION ANALYSIS (ResNet18) ###\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"Objective: Improve generalization on the 10-shot dataset using MixUp.\")\n",
    "    report.append(f\"Baseline (10-shot, no MixUp): {baseline_acc:.2f}%\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    if 'mixup' in results and results['mixup']:\n",
    "        report.append(\"Results:\")\n",
    "        best_alpha = None\n",
    "        best_acc = 0\n",
    "        \n",
    "        for alpha_name, history in sorted(results['mixup'].items()):\n",
    "            alpha_val = history['alpha']\n",
    "            test_acc = history['test_acc']\n",
    "            val_acc = history['best_val_acc']\n",
    "            improvement = test_acc - baseline_acc\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_alpha = alpha_val\n",
    "            \n",
    "            report.append(f\"  Alpha = {alpha_val}:\")\n",
    "            report.append(f\"    Best val accuracy: {val_acc:.2f}%\")\n",
    "            report.append(f\"    Test accuracy: {test_acc:.2f}%\")\n",
    "            report.append(f\"    Change from baseline: {'+' if improvement >= 0 else ''}{improvement:.2f}%\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        report.append(\"Key Findings:\")\n",
    "        report.append(f\"  • Best alpha value: {best_alpha} (Test Acc: {best_acc:.2f}%)\")\n",
    "        report.append(\"  • MixUp acts as a regularizer. By creating interpolated images and labels,\")\n",
    "        report.append(\"    it forces the model to learn smoother decision boundaries, reducing overfitting.\")\n",
    "        report.append(\"  • This technique appears to be effective, providing a performance boost.\")\n",
    "        report.append(\"\")\n",
    "    else:\n",
    "        report.append(\"No MixUp results to analyze.\\n\")\n",
    "    \n",
    "    # Triplet Loss Analysis \n",
    "    report.append(\"### 3. TRIPLET LOSS ANALYSIS (ResNet18) ###\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"Objective: Evaluate combined metric learning (Triplet) and classification (Cross-Entropy) loss.\")\n",
    "    report.append(f\"Baseline (cross-entropy only): {baseline_acc:.2f}%\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    if 'triplet' in results and results['triplet']:\n",
    "        history = results['triplet']\n",
    "        test_acc = history['test_acc']\n",
    "        val_acc = history['best_val_acc']\n",
    "        improvement = test_acc - baseline_acc\n",
    "        \n",
    "        report.append(\"Results:\")\n",
    "        report.append(f\"  Margin: {history['margin']}\")\n",
    "        report.append(f\"  Lambda (triplet weight): {history['lambda_triplet']}\")\n",
    "        report.append(f\"  Best val accuracy: {val_acc:.2f}%\")\n",
    "        report.append(f\"  Test accuracy: {test_acc:.2f}%\")\n",
    "        report.append(f\"  Change from baseline: {'+' if improvement >= 0 else ''}{improvement:.2f}%\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        report.append(\"Key Findings:\")\n",
    "        report.append(\"  • The combined loss (Triplet + Cross-Entropy) forces the model to learn a more\")\n",
    "        report.append(\"    discriminative feature space. This is ideal for fine-grained tasks like flower recognition.\")\n",
    "        report.append(\"  • This approach directly optimizes for inter-class separation (pushing negatives away)\")\n",
    "        report.append(\"    and intra-class compactness (pulling positives closer), resulting in a strong performance gain.\")\n",
    "        report.append(\"\")\n",
    "    else:\n",
    "        report.append(\"No Triplet Loss results to analyze.\\n\")\n",
    "\n",
    "    # Overall Summary \n",
    "    report.append(\"### 4. OVERALL SUMMARY ###\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Find best technique\n",
    "    best_technique = \"Baseline\"\n",
    "    best_accuracy = baseline_acc\n",
    "    \n",
    "    if 'mixup' in results and results['mixup']:\n",
    "        for alpha_name, history in results['mixup'].items():\n",
    "            if history['test_acc'] > best_accuracy:\n",
    "                best_accuracy = history['test_acc']\n",
    "                best_technique = f\"MixUp (alpha={history['alpha']})\"\n",
    "    \n",
    "    if 'triplet' in results and results['triplet']:\n",
    "        if results['triplet']['test_acc'] > best_accuracy:\n",
    "            best_accuracy = results['triplet']['test_acc']\n",
    "            best_technique = \"Triplet Loss\"\n",
    "    \n",
    "    report.append(f\"Best Technique (Person C): {best_technique}\")\n",
    "    report.append(f\"Best Test Accuracy: {best_accuracy:.2f}%\")\n",
    "    report.append(f\"Improvement over Person A baseline: {best_accuracy - baseline_acc:+.2f}%\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"=\"*80)\n",
    "    \n",
    "    # Print and save report\n",
    "    report_text = \"\\n\".join(report)\n",
    "    print(report_text)\n",
    "    \n",
    "    with open('results/person_c/analysis_report.txt', 'w') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    return report_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4eda14",
   "metadata": {},
   "source": [
    "## Part 7: Main Execution\n",
    "\n",
    "This is the main entry point to run all experiments. \n",
    "\n",
    "**Note:** Set `quick_test=True` for a fast 3-5 epoch run to verify the code works. \n",
    "Set `quick_test=False` for the full, final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to False for full run, True for a quick test\n",
    "QUICK_TEST = True \n",
    "\n",
    "# Load baseline results to pass to report\n",
    "# (Assuming Person A's baseline is 79.67% and Person B's is 85.51%)\n",
    "baseline_acc_a = 79.67\n",
    "baseline_acc_b = 85.51\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERSON C - ADVANCED TECHNIQUES MODULE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run experiments\n",
    "results = run_all_experiments(quick_test=QUICK_TEST)\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "if 'fewshot' in results and results['fewshot']:\n",
    "    # Use the 10-shot result as the baseline for the plot\n",
    "    baseline_10_shot_acc = results['fewshot'].get('10_shot', {}).get('test_acc', baseline_acc_a)\n",
    "    plot_fewshot_comparison(results['fewshot'], baseline_acc=baseline_10_shot_acc)\n",
    "\n",
    "if 'mixup' in results and results['mixup']:\n",
    "    plot_mixup_comparison(results['mixup'], baseline_acc=baseline_acc_a)\n",
    "\n",
    "plot_all_techniques_comparison(results, baseline_acc=baseline_acc_a)\n",
    "\n",
    "# Generate comparison table\n",
    "create_comparison_table(results, baseline_acc=baseline_acc_a, person_b_best_acc=baseline_acc_b)\n",
    "\n",
    "# Generate detailed analysis report\n",
    "generate_analysis_report(results, baseline_acc=baseline_acc_a)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERSON C EXPERIMENTS COMPLETE!\")\n",
    "print(\"Results saved in: results/person_c/\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
